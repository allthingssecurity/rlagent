#!/usr/bin/env python3
"""
Training script with LoRA bypass to avoid the lora_tensors error.
This disables LoRA entirely and uses base model training.
"""

import os
import asyncio
import random

# DISABLE LORA BEFORE ANY IMPORTS
os.environ["VLLM_DISABLE_LORA"] = "1"
os.environ["VLLM_DISABLE_CUSTOM_ALL_REDUCE"] = "1"
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"

from dotenv import load_dotenv
load_dotenv()

import torch

# Ensure GPU mode
if not torch.cuda.is_available():
    print("âŒ CUDA not available! This script requires GPU.")
    exit(1)

print("ğŸ”¥ GPU Training with LoRA Bypass")
print("=" * 50)
print("ğŸš« LoRA disabled to avoid compatibility issues")
print(f"ğŸ–¥ï¸ GPU: {torch.cuda.get_device_name(0)}")
print(f"ğŸ”¥ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
print("=" * 50)

import art
from art.local import LocalBackend
from art.trajectories import Trajectory, TrajectoryGroup

# Configuration
MODEL_NAME = "gpu-no-lora"
PROJECT = "gpu-bypass"
BASE_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"  # Smaller model without LoRA
TRAIN_STEPS = 3
BATCH_SIZE = 4
USE_RULER = bool(os.getenv("OPENAI_API_KEY"))

print(f"ğŸ“‹ Model: {MODEL_NAME}")
print(f"ğŸ§  Base Model: {BASE_MODEL}")
print(f"ğŸ‹ï¸ Training Steps: {TRAIN_STEPS}")
print(f"ğŸ“¦ Batch Size: {BATCH_SIZE}")
print(f"ğŸš« LoRA: Disabled")
print("=" * 50)


def generate_problems(num_problems: int = 10):
    """Generate simple problems for no-LoRA training."""
    problems = []
    
    for i in range(num_problems):
        # Simple math problems
        a, b = random.randint(1, 20), random.randint(1, 20)
        question = f"What is {a} + {b}?"
        answer = a + b
        solution = f"{a} + {b} = {answer}"
        
        messages = [
            art.types.Message(
                role="system", 
                content="You are a math helper. Answer briefly and clearly."
            ),
            art.types.Message(
                role="user", 
                content=question
            ),
            art.types.Message(
                role="assistant", 
                content=solution
            )
        ]
        
        trajectory = Trajectory(
            messages=messages,
            reward=1.0,
            metadata={
                "question": question,
                "answer": answer,
                "no_lora": True
            }
        )
        
        problems.append(trajectory)
    
    return problems


async def no_lora_rollout(model, problem_data, step_num):
    """Simple rollout without LoRA complications."""
    trajectories = []
    
    for i, problem in enumerate(problem_data[:BATCH_SIZE]):
        messages = [
            art.types.Message(
                role="system", 
                content="You are helpful with math."
            ),
            problem.messages[1],  # Question
            problem.messages[2]   # Answer
        ]
        
        trajectory = Trajectory(
            messages=messages,
            reward=problem.reward,
            metadata={
                **problem.metadata,
                "step": step_num,
                "batch_index": i,
                "bypass_mode": True
            }
        )
        
        trajectories.append(trajectory)
    
    return trajectories


async def main():
    """Main training function with LoRA bypass."""
    random.seed(42)
    
    try:
        print("\nğŸ”§ Initializing ART backend (LoRA disabled)...")
        backend = LocalBackend()
        
        print("ğŸ“ Creating model without LoRA...")
        model = art.TrainableModel(
            name=MODEL_NAME,
            project=PROJECT,
            base_model=BASE_MODEL,
        )
        
        # Simple configuration without LoRA
        model._internal_config = art.dev.InternalModelConfig(
            init_args=art.dev.InitArgs(
                max_seq_length=1024,  # Smaller for stability
            ),
        )
        
        print("ğŸ“‹ Registering model...")
        print("â³ This should work without LoRA errors...")
        
        await model.register(backend)
        print("âœ… Model registered successfully!")
        
        current_step = await model.get_step()
        print(f"ğŸ“ˆ Current model step: {current_step}")
        
        print(f"\nğŸ“Š Generating {10} simple problems...")
        problem_data = generate_problems(10)
        print(f"âœ… Generated {len(problem_data)} problems")
        
        # Show sample
        sample = problem_data[0]
        print(f"\nğŸ“ Sample: {sample.metadata['question']} â†’ {sample.metadata['answer']}")
        
        print(f"\nğŸš€ Starting no-LoRA training for {TRAIN_STEPS} steps...")
        
        # Training loop
        for step in range(current_step, current_step + TRAIN_STEPS):
            print(f"\nğŸ‹ï¸ Training Step {step + 1}/{current_step + TRAIN_STEPS}")
            print("â”€" * 30)
            
            try:
                # Generate trajectories
                print("   ğŸ“Š Generating trajectories...")
                trajectories = await no_lora_rollout(model, problem_data, step)
                print(f"   âœ… Generated {len(trajectories)} trajectories")
                
                # Create trajectory groups
                trajectory_groups = [TrajectoryGroup(trajectories=trajectories)]
                print("   ğŸ“¦ Created trajectory groups")
                
                # Train without LoRA
                print("   ğŸ‹ï¸ Training model (no LoRA)...")
                train_config = art.TrainConfig(learning_rate=1e-4)  # Conservative LR
                
                training_completed = False
                async for metrics in model.train(trajectory_groups, config=train_config):
                    if 'loss' in metrics:
                        print(f"      ğŸ“‰ Loss: {metrics['loss']:.4f}")
                    if 'learning_rate' in metrics:
                        print(f"      ğŸ“ˆ LR: {metrics['learning_rate']:.2e}")
                    
                    training_completed = True
                    break  # Just one iteration per step
                
                if training_completed:
                    print(f"   âœ… Step {step + 1} completed!")
                    
                    # Show GPU memory
                    if torch.cuda.is_available():
                        memory_used = torch.cuda.memory_allocated() / 1e9
                        print(f"   ğŸ”¥ GPU Memory: {memory_used:.1f}GB used")
                else:
                    print(f"   âš ï¸ Step {step + 1} had issues")
                
            except Exception as e:
                print(f"   âŒ Step failed: {e}")
                print(f"   ğŸ” Error type: {type(e).__name__}")
                
                # Check if it's still the LoRA error
                if 'lora_tensors' in str(e).lower():
                    print("   ğŸš« Still hitting LoRA error - trying alternative approach")
                    break
                else:
                    print("   ğŸ”„ Different error - continuing...")
                    continue
        
        final_step = await model.get_step()
        print(f"\nğŸ‰ Training completed!")
        print(f"ğŸ“ˆ Final step: {final_step}")
        print(f"ğŸš« LoRA bypass mode worked!")
        
    except Exception as e:
        print(f"\nâŒ Training failed: {e}")
        print(f"ğŸ” Error details: {type(e).__name__}")
        
        if 'lora_tensors' in str(e).lower():
            print("\nğŸ’¡ Still hitting LoRA compatibility issue!")
            print("ğŸ”§ Possible solutions:")
            print("   1. Run: python fix_lora_tensors.py")
            print("   2. Try even older versions: vLLM 0.2.5, PEFT 0.4.0")
            print("   3. Use pure CPU training: python train_simple_cpu.py")
        else:
            print(f"\nğŸ’¡ Different error - this is progress!")
            print("The LoRA bypass worked, but hit a different issue")
        
        raise
    
    finally:
        # Cleanup
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        if 'backend' in locals() and backend is not None:
            try:
                backend.close()
            except Exception as e:
                print(f"Warning: Cleanup error: {e}")


if __name__ == "__main__":
    print("ğŸš€ Starting LoRA bypass training...")
    asyncio.run(main())